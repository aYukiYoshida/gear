仮説検定と考え方

	統計学的処理によってある仮説(Hypothesis)が正しいかどうか調べる方法
	正当性(それっぽさ)を棄却する仮説を帰無仮説(Null Hypothesis)という
	逆に正当性を証明しようとする仮説は対立仮説(Alternative Hypothesis)
	帰無仮説を棄却出来れば対立仮説は正しい(それっぽい)というスタンス
	証明しようとする対立仮説が正しければ帰無仮説は棄却される事になる
	もっと簡単に言えば、ダメな確率が低ければダメじゃないってことだろう

	判断は優位水準(危険率)を指標として、だいたい0.05か0.01が使用される
	帰無仮説を指示する確率(Probability)が優位水準より高ければ棄却不可能
	Null Hypothesisの確率が高ければ仮説は微妙にあるかもしれないってこと
	逆にいえばNull Hypothesisの確率が低ければその仮説はありえねぇだろう
	なんとも複雑だが、この「仮説」とやらは何を目的とするかで違ってくる
	この確率はモデルを「棄却」する事は出来るが「肯定」する事は出来ない
	確率が高くでた場合も、水準ギリの場合も棄却不可能という意味では同じ
	つまり、高い確率で棄却出来るからといって正しいかは別問題ということ

	Scientificな証明とは、一般的にNull Hypothesisを否定するという方式
	Null Hypothesisを棄却できれば、当初の主張は証明されたという背理法
	主張と反対の仮説を棄却できれば、主張が正しい可能性はきわめて高い
	「Aである事」を証明するには、「Aでない可能性」を棄却してやればよい
	忘れてはいけないのは、この検定から「真実」がわかるわけではない事
	この方法では、統計的に有意に仮説を棄却できるという事が言えるだけ
	はじめの仮説(モデル)が真である、といっているわけではない事に注意

例えば...

	新薬Aと偽薬Bの効能を比べて、新薬Aの効能を証明したいとする
	新薬Aの方が効果的、という証拠を並べても科学的に不十分と判断される
	代わりに「新薬Aと偽薬Bは全く同じ効果の薬」という帰無仮説をたてる
	実際に新薬Aと偽薬Bで治験を行い、データをもとに統計処理を行う
	こうすれば「新薬Aと偽薬Bが同じ確率は??%」という形で数字が出せる
	例えば「AとBが同じものである確率が5%以下」という結果が出たとする
	すると「95%以上の確率で新薬Aは偽薬Bとは効果が違う」と言える
	つまり、AとBは同じであると仮定した帰無仮説を棄却できたことになる
	こうして「統計的に有意に違いが確認できた」ということが言える

信頼区間と優位水準

	90%の信頼区間(Confidence Level)で検出されました...ってどゆことや？！
	信頼係数(Confidence Coefficient) = 1 - 優位水準(Significance Level)
	エラーの範囲を表示する場合は一般的に 90% confidence level を使用する
	エラーの範囲表示でよく使用されるのは 90%、95%、99% のどれかだろう
	検出されたかどうかの目安は信頼区間でなく Sigma を使用する事が多い
	検出された事を強く主張する為には 3-Sigma 以上というのが一般的な見解
	検出が 2-Sigma ならまぁ高確率、1-Sigmaなら Suggestion 程度が妥当な所

	それぞれの対応は以下の様になっている
	90.00% confidence level = 10% significance level detection
	90.00% confidence level = 1.64-Sigma		 detection
	95.00% confidence level = 1.96-Sigma		 detection
	99.00% confidence level = 2.58-Sigma		 detection
	68.27% confidence level = 1.00-Sigma		 detection
	95.45% confidence level = 2.00-Sigma		 detection
	99.73% confidence level = 3.00-Sigma		 detection
	99.99% confidence level = 4.00-Sigma		 detection

        90%の信頼区間とはバイアスが無ければ母集団の推定値が90回存在する分布幅
	別の言い方をすれば、実験を100回やれば90回はその値が出る分布幅ってこと
	1-Sigma検出とは同じ天体を3回観測したら2回くらいそうなりますよってこと
	信頼区間(Confidence Interval)の両端は信頼限界(Confidence Limit)という
	パラメータの数が1つでない場合は変わったりするので注意する (TBW)

カイ二乗検定

	たとえばXSPECでFitする場合、対立仮説に"モデルが正しくない"としている
	すると、帰無仮説は"モデルが正しい"ということを意味する (ほんとか？)
	結果的に Null Hypothesis Probability が小さいほどモデルは正しくない

	Chi-Squared                 = Σ((観測値-誤差)^2)/誤差
	Degrees of Freedom          = データ数 - フリーパラメータ数
	Reduced chi-squared         = Chi-Squared/D.o.F. (だいたい1になればOK)
	Null Hypothesis Probability = 棄却出来ない確率
				      高ければ棄却出来ないそれっぽいモデル
				      低ければ棄却出来るそれっぽくないモデル
				      この場合は優位確率と同じようなもんだな

	各パラメータ信頼度によるコントアを書く場合はモデルが正しいと仮定する
	値を動かすと最小Chi-Squaredからどれだけ変動するかというコントアが正解
	Null Hypothesis Probabilityのコントアは正しそうだが、実は間違っている
	それぞれのパラメータ信頼度を出すにはモデルが正しいという前提が必要だ
	誤差範囲を求めるには、パラメータを動かしてChi-Squaredの値の動きを見る
	1つのパラメータを動かして値が 2.706 悪くなった所が90%信頼度の誤差範囲

F検定 (F-Test)

	2つの正規母集団のばらつき(母分散)に差があるかを調べる統計学的な検定法
	2つの正規母集団から抽出する標本の数は違っていても良い
	正規母集団の分散の平均は既知のものとしてF検定を実施する
	帰無仮説は"2つの正規母集団の分散の平均は等しい"というものになる

	F statistic value	: 大きかったらモデルは優位 (F=s1^{2}/s2^{2})
				  まったく同じモデルを検定すれば F=1 になる
				  大きければ分布は別物、つまり優位ってこと
	Probability		: 小さかったらモデルは優位
				  P=0.1 なら 90% Confidence Level という意味

	ただし！輝線の有無を議論する場合に F-Test を使用するのは正しくない
	詳しくは http://adsabs.harvard.edu/abs/2002ApJ...571..545P 参照

モデルや輝線の存在確率を検定する方法 (A)

	XSPECの error コマンドを使用すれば存在確率が直接的に計算できる

	A-1. XSPECでモデルに対するNorm.のエラーを求める (これは1-Sigmaとなる)
	     他のパラメータは全てBest-Fit値で固定しておいた方が良い
	A-2. Best-Fit値からエラーの3倍を引き算して0以上なら 3-Sigma で検出
	A-3. 論文にはやり方をちゃんと書いておけばそれで問題ない
	     輝線エネルギーを固定した場合「?.?keVの輝線を仮定すれば...」でよい

モデル(や輝線)のモデルの存在確率を検定する方法 (B)

	XSPECの ftest コマンドでF検定により存在確率を計算できる
	ちなみに輝線の存在確率をこの方法で検定するのは本来間違っているらしい
	http://heasarc.gsfc.nasa.gov/lheasoft/xanadu/xspec/manual/XSftest.html
	ちなみにF検定は別のデータに対して処理するのが本流らしい (森井,河合談)

	B-1. XSPECでモデルを入れた場合と入れない場合でChi^{2}値を求める
	     この場合になるべく関係ないパラメータは固定しておくと良い
	B-2. F検定で出てきた値がそのまま存在確率を表す

モデルや輝線の存在確率を検定する方法 (C)

	F分布をモンテカルロでシミュレーションしてより正確な存在確率を計算する
	輝線の存在確率を正しく見積もる為に編みだしたらしい、方法は以下参照

	Protassov et al. 2002, ApJ, 571, 545
	http://adsabs.harvard.edu/abs/2002ApJ...571..545P

	NullモデルをPower、調査モデルをPower+GaussとしてGaussの優位性を見る

	C-1. XSPECのfakeitで観測とまったく同じ条件でNullモデルをFakeしまくる
	     上の論文では2000回くらいシミュレーションしていた (以下N個とする)
	C-2. N個のシミュレーションをそれぞれ観測データと同じBinまとめにする
	     要するに観測とまったく同じFakeを統計取れるくらい多く作れってこと
	C-3. Nullモデルと調査モデルでN個のFakeをFitして F-Statistic 値を求める
	     これでN個の F-Statistic 値によるヒストグラムが出来上がる
	C-4. 観測データからも同じようにして F-Statistic 値を求めておく
	C-5. PPP-Value = (1/N)*Sigma_{t=1}^{N}(I[Tf(x,t)>Tf(x)]) を計算する
		  Tf(x)	   : 観測データから求めた F-Statistic 値
		  Tf(x,t)  : Fakeデータから求めた F-Statistic 値
		  I[f(x)]  : True(Fakeが大)ならI=1、Fales(Fakeが小)ならI=0
	C-6. 求めた PPP-Value が最終的に F-Test の P-Value と同じ意味を持つ
	     P-Valueがより小さいほど複雑なモデル = つまりGaussがある事を示す

	F値はGaussが効果的なほど大きくなる = Fakeは統計が良ければ小さくなる
	例えば PPP-Value が小さい場合 = F値はFakeの方がほとんど小さい
	もし統計揺らぎが大きかったら、F値が大きく出てしまうのもあるかも
	大きく出るやつに実際のF値が埋もれている場合、揺らぎの可能性がデカい
	逆にFakeのF値に埋もれない場合、揺らぎでは起こり得ない事を意味する
	よって、この場合は Gauss が存在すると言ってもよいだろう
	PPP-Valueは求めたF値より高い値にどれだけの割合でなるか見積もった値

	この様な自分と比較する方法を「Parametric Bootstrap (Method)」と呼ぶ
	F分布、LRT分布に従うか不明なので、F-TestやLRT-Testだと正しくないはず
	この方法なら分布をFakeからブートストラップで導いているので良さそうだ

